{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"dataset/train_original_tweets_top1000full.json\")\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup_text(text):\n",
    "    # remove &quot and &amp\n",
    "    text = re.sub(r'&quot;(.*?)&quot;', \"\\g<1>\", text)\n",
    "    text = re.sub(r'&amp;', \"\", text)\n",
    "\n",
    "    # replace emoticon\n",
    "    text = re.sub(r'(^| )(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)', \"\\g<1>TOKEMOTICON\", text)\n",
    "\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"tokemoticon\", \"TOKEMOTICON\")\n",
    "\n",
    "    # replace url\n",
    "    text = re.sub(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?',\n",
    "                \"TOKURL\", text)\n",
    "\n",
    "    # replace mention\n",
    "    text = re.sub(r'@[\\w]+', \"TOKMENTION\", text)\n",
    "\n",
    "    # replace hashtag\n",
    "    text = re.sub(r'#[\\w]+', \"TOKHASHTAG\", text)\n",
    "\n",
    "    # replace dollar\n",
    "    text = re.sub(r'\\$\\d+', \"TOKDOLLAR\", text)\n",
    "\n",
    "    # remove punctuation\n",
    "    text = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
    "\n",
    "    # remove special word\n",
    "    text = re.sub('TOKDDOLLAR', ' ', text)\n",
    "\n",
    "    # TODO: test if they are useful in prediction\n",
    "    text = re.sub('TOKMENTION', ' ', text)\n",
    "    text = re.sub('TOKEMOTICON', ' ', text)\n",
    "    text = re.sub('TOKHASHTAG', ' ', text)\n",
    "    text = re.sub('TOKURL', ' ', text)\n",
    "\n",
    "    # remove multiple spaces\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "\n",
    "    # remove newline\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(text, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    clean_text = cleanup_text(text)\n",
    "    tokens = nltk.word_tokenize(clean_text)\n",
    "    lemma_tokens = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            lemma_tokens.append(str(lemmatizer.lemmatize(token)))\n",
    "        except:\n",
    "            pass\n",
    "    return lemma_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_all(df, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    df[\"text\"] = list(map(lambda x: process(x, lemmatizer), df[\"text\"].values))\n",
    "    df[\"len\"] = df[\"text\"].apply(len)\n",
    "    df[\"label\"] = df[\"retweet_count\"] > df[\"retweet_median\"]\n",
    "    df[\"label\"] = df[\"label\"].astype('int')\n",
    "    df = df[['label', 'screen_name', 'text', 'len']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_model(file_name):\n",
    "    # GloVe Model\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(file_name, binary=False) \n",
    "    return model\n",
    "\n",
    "glove_model = load_model(\"../glove.twitter.27B/glove.twitter.27B.25d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_df = process_all(df, lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of         label      screen_name  \\\n",
       "0           1        katyperry   \n",
       "1           1        katyperry   \n",
       "10          0        katyperry   \n",
       "100         1        katyperry   \n",
       "1000        1     TheEllenShow   \n",
       "10000       0             espn   \n",
       "100000      0  harbhajan_singh   \n",
       "100001      0  harbhajan_singh   \n",
       "100002      0  harbhajan_singh   \n",
       "100003      1  harbhajan_singh   \n",
       "100004      0  harbhajan_singh   \n",
       "100005      0  harbhajan_singh   \n",
       "100006      0  harbhajan_singh   \n",
       "100007      0  harbhajan_singh   \n",
       "100008      0  harbhajan_singh   \n",
       "100009      0  harbhajan_singh   \n",
       "10001       0             espn   \n",
       "100010      0  harbhajan_singh   \n",
       "100011      0  harbhajan_singh   \n",
       "100012      0  harbhajan_singh   \n",
       "100013      0  harbhajan_singh   \n",
       "100014      0  harbhajan_singh   \n",
       "100015      0  harbhajan_singh   \n",
       "100016      1  harbhajan_singh   \n",
       "100017      0  harbhajan_singh   \n",
       "100018      0  harbhajan_singh   \n",
       "100019      0  harbhajan_singh   \n",
       "10002       1             espn   \n",
       "100020      1  harbhajan_singh   \n",
       "100021      1  harbhajan_singh   \n",
       "...       ...              ...   \n",
       "99972       1  harbhajan_singh   \n",
       "99973       0  harbhajan_singh   \n",
       "99974       1  harbhajan_singh   \n",
       "99975       1  harbhajan_singh   \n",
       "99976       0  harbhajan_singh   \n",
       "99977       1  harbhajan_singh   \n",
       "99978       0  harbhajan_singh   \n",
       "99979       1  harbhajan_singh   \n",
       "9998        0             espn   \n",
       "99980       1  harbhajan_singh   \n",
       "99981       1  harbhajan_singh   \n",
       "99982       0  harbhajan_singh   \n",
       "99983       0  harbhajan_singh   \n",
       "99984       0  harbhajan_singh   \n",
       "99985       1  harbhajan_singh   \n",
       "99986       1  harbhajan_singh   \n",
       "99987       0  harbhajan_singh   \n",
       "99988       0  harbhajan_singh   \n",
       "99989       0  harbhajan_singh   \n",
       "9999        1             espn   \n",
       "99990       1  harbhajan_singh   \n",
       "99991       1  harbhajan_singh   \n",
       "99992       0  harbhajan_singh   \n",
       "99993       0  harbhajan_singh   \n",
       "99994       0  harbhajan_singh   \n",
       "99995       0  harbhajan_singh   \n",
       "99996       1  harbhajan_singh   \n",
       "99997       0  harbhajan_singh   \n",
       "99998       1  harbhajan_singh   \n",
       "99999       0  harbhajan_singh   \n",
       "\n",
       "                                                     text  len  \n",
       "0       [i, can, t, wait, for, you, to, witness, my, b...   20  \n",
       "1       [who, s, ready, for, live, on, instagram, from...   23  \n",
       "10      [we, are, listening, and, we, like, what, we, ...    9  \n",
       "100     [this, just, in, now, your, little, one, can, ...   29  \n",
       "1000    [happy, birthday, jim, parson, or, a, i, call,...   11  \n",
       "10000   [it, s, espn, the, magazine, s, 20th, annivers...   32  \n",
       "100000  [it, wa, a, comedy, show, not, dance, show, it...   29  \n",
       "100001             [thanks, shera, man, neeva, mat, uchi]    6  \n",
       "100002    [singh, is, king, sardar, ji, looking, dashing]    7  \n",
       "100003  [yeah, remember, this, full, masti, dk, that, ...   19  \n",
       "100004       [morning, friend, have, a, good, day, ahead]    7  \n",
       "100005                             [thank, you, champion]    3  \n",
       "100006                             [thank, you, veer, ji]    4  \n",
       "100007                                      [thanks, sid]    2  \n",
       "100008                                       [thank, lax]    2  \n",
       "100009                             [thanks, chotte, veer]    3  \n",
       "10001                                 [this, is, madness]    3  \n",
       "100010                                     [thanks, paji]    2  \n",
       "100011                                  [thanks, brother]    2  \n",
       "100012                                  [thanks, brother]    2  \n",
       "100013                             [thanks, chotte, veer]    3  \n",
       "100014                                       [thanks, rp]    2  \n",
       "100015                                  [thanks, brother]    2  \n",
       "100016  [thank, you, paji, for, your, blessing, trying...   15  \n",
       "100017                    [thank, you, brother, zindabad]    4  \n",
       "100018                      [thank, you, so, much, gauti]    5  \n",
       "100019  [thanks, wifey, for, always, supporting, and, ...    8  \n",
       "10002                                             [march]    1  \n",
       "100020  [ik, suneha, part, 2, is, now, released, need,...   20  \n",
       "100021  [looking, forward, to, release, this, today, f...   21  \n",
       "...                                                   ...  ...  \n",
       "99972     [kaala, teaser, with, a, twist, check, it, out]    8  \n",
       "99973   [congratulation, very, happy, for, you, guy, s...    8  \n",
       "99974     [whaaattttt, a, ride, to, chepauk, whitslepodu]    6  \n",
       "99975   [going, for, our, first, practice, session, in...   11  \n",
       "99976                       [god, bless, you, advik, kid]    5  \n",
       "99977   [happy, birthday, wish, you, all, the, best, m...   17  \n",
       "99978                                  [thank, you, paji]    3  \n",
       "99979   [about, last, night, congratulation, wish, you...   11  \n",
       "9998    [michigan, s, locker, room, wa, hype, after, j...   13  \n",
       "99980   [so, happy, to, be, with, the, team, behind, o...   11  \n",
       "99981   [wow, wow, great, treatment, nd, fairplay, no,...   46  \n",
       "99982      [congratulation, test, wicket, keep, spinning]    5  \n",
       "99983   [happy, birthday, have, a, good, one, stay, ha...   13  \n",
       "99984                        [always, good, to, see, you]    5  \n",
       "99985   [yeah, this, will, be, talked, about, not, jus...   13  \n",
       "99986        [aa, gya, shera, wapas, good, luck, brother]    7  \n",
       "99987                                      [cheer, mitch]    2  \n",
       "99988                  [thank, you, anna, see, you, soon]    6  \n",
       "99989                               [thank, you, brother]    3  \n",
       "9999    [braden, halladay, son, of, roy, halladay, who...   25  \n",
       "99990                                       [dance, move]    2  \n",
       "99991   [27, yes, a, very, special, number, for, me, n...   16  \n",
       "99992   [we, always, take, our, natural, resource, for...   36  \n",
       "99993   [what, a, day, that, wa, and, what, a, feeling...   26  \n",
       "99994   [thank, you, my, brother, love, you, let, s, m...   18  \n",
       "99995   [always, lovely, catching, up, with, you, mann...   12  \n",
       "99996   [brother, i, love, marathi, too, i, am, a, pro...   39  \n",
       "99997   [thank, you, brother, bhagat, singh, zindabad,...    9  \n",
       "99998   [happy, to, be, here, in, kolkota, to, launch,...   30  \n",
       "99999                                   [thanks, brother]    2  \n",
       "\n",
       "[214405 rows x 4 columns]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df['len'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def creat_feature(text, model, dim=25, max_len=40):\n",
    "    '''\n",
    "    Create the sentence matrix as the input of LSTM cell\n",
    "    We padding the first dim of the matrix to max_len\n",
    "    '''\n",
    "    # You can try concatenation, simple summation, pointwise multiplication, convolution etc. \n",
    "    feature = np.zeros((max_len, dim))\n",
    "    for i in range(len(text)):\n",
    "        try:\n",
    "            feature[i, :] = model[text[i]]\n",
    "        except:\n",
    "            feature[i, :] = model[',']\n",
    "    for i in range(len(text), max_len):\n",
    "        feature[i, :] = model[',']\n",
    "    feature = np.squeeze(feature)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, label_size, embedding_dim, \n",
    "                 lstm_hidden_size, lstm_num_layers, dropout):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            batch_first=True,\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_layers=lstm_num_layers,\n",
    "            dropout=dropout,\n",
    "            bidirectional=False)\n",
    "        self.dense = nn.Linear(\n",
    "            in_features=lstm_hidden_size,\n",
    "            out_features=label_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, embedding):\n",
    "        # embedding (N, W, H)\n",
    "        out_lstm, _ = self.lstm(embedding)\n",
    "        out_lstm = out_lstm.permute(1, 0, 2)  # (W, N, C*H)\n",
    "        out_linear = self.dense(out_lstm[-1])\n",
    "        output = self.softmax(out_linear)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_frac = 0.7\n",
    "\n",
    "data = processed_df['text'].apply(lambda x: creat_feature(x, glove_model, dim=25, max_len=70)).values\n",
    "label = processed_df['label'].values\n",
    "\n",
    "data_size = len(processed_df)\n",
    "train_data = data[:(int)(train_frac * data_size)]\n",
    "train_label = label[:(int)(train_frac * data_size)]\n",
    "val_data = data[(int)(train_frac * data_size):]\n",
    "val_label = label[(int)(train_frac * data_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "embedding_dim=25\n",
    "lstm_hidden_size=32\n",
    "lstm_num_layers=3\n",
    "dropout=0.2\n",
    "\n",
    "label_size = 2\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = LSTMClassifier(label_size, embedding_dim,\n",
    "                       lstm_hidden_size, lstm_num_layers, dropout)\n",
    "model = model.cuda() if use_cuda else model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "torch.manual_seed(2018)\n",
    "for epoch in range(10):\n",
    "    print(\"epoch: \" + str(epoch))\n",
    "    for batch_index in range(0, len(train_data), batch_size):\n",
    "        model.zero_grad()\n",
    "        batch_data = train_data[batch_index: batch_index + batch_size]\n",
    "        batch_label = train_label[batch_index: batch_index + batch_size]\n",
    "        batch_data = np.stack(batch_data, axis=0)\n",
    "   \n",
    "        train_x = torch.from_numpy(batch_data).float()\n",
    "        train_y = torch.from_numpy(batch_label)\n",
    "\n",
    "        if use_cuda:\n",
    "            train_x = train_x.cuda()\n",
    "            train_y = train_y.cuda()\n",
    "        \n",
    "        train_x = Variable(train_x)\n",
    "        train_y = Variable(train_y)\n",
    "        \n",
    "        y_pred = model(train_x)\n",
    "        \n",
    "        loss = loss_fn(y_pred, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if use_cuda:\n",
    "            print(\"batch_range:[\"+str(batch_index)+\",\"+str(batch_index+batch_size)+\"),loss:\"+str(loss.cpu().numpy()[0]))\n",
    "        else:\n",
    "            print(\"batch_range:[\"+str(batch_index)+\",\"+str(batch_index+batch_size)+\"),loss:\"+str(loss.data.numpy()[0]))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_index in range(0, len(val_data), batch_size):\n",
    "    batch_data = val_data[batch_index: batch_index + batch_size]\n",
    "    batch_label = val_label[batch_index: batch_index + batch_size]\n",
    "    batch_data = np.stack(batch_data, axis=0)\n",
    "\n",
    "    val_x = torch.from_numpy(batch_data).float()\n",
    "\n",
    "    if use_cuda:\n",
    "        val_x = val_x.cuda()\n",
    "    val_x = Variable(val_x)\n",
    "\n",
    "    y_pred = model(val_x)\n",
    "    _, label_pred = torch.max(y_pred, 1)\n",
    "    print(\"accuracy:%f\" % (sum(label_pred.data.numpy() == batch_label)/float(len(batch_label))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

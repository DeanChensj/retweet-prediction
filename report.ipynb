{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "from nltk.tokenize import word_tokenize\n",
    "import data_io,SIF_embedding\n",
    "import scipy\n",
    "import kmeans\n",
    "import natural_language_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                created_at                                           entities  \\\n",
      "0      2018-04-07 04:06:52  {'urls': [{'url': 'https://t.co/s0xEcWFiqn', '...   \n",
      "1      2018-04-07 03:54:20  {'urls': [{'url': 'https://t.co/Dqgw5v9a13', '...   \n",
      "10     2018-04-03 04:02:16  {'urls': [], 'hashtags': [{'text': 'americanid...   \n",
      "100    2018-03-20 04:14:46  {'urls': [], 'hashtags': [], 'user_mentions': []}   \n",
      "1000   2018-03-05 02:35:43  {'urls': [], 'hashtags': [], 'user_mentions': []}   \n",
      "10000  2018-03-23 01:10:56  {'urls': [{'url': 'https://t.co/Ma4hOcN0TO', '...   \n",
      "100000 2018-03-09 15:32:47  {'urls': [{'url': 'https://t.co/ASleT4U6tP', '...   \n",
      "100001 2018-03-09 15:28:54  {'urls': [], 'hashtags': [], 'user_mentions': ...   \n",
      "100002 2018-03-09 15:12:49  {'urls': [{'url': 'https://t.co/ASleT4U6tP', '...   \n",
      "100003 2018-03-09 15:02:37  {'urls': [{'url': 'https://t.co/rn1fmdQFY7', '...   \n",
      "100004 2018-03-09 14:27:59  {'urls': [{'url': 'https://t.co/ASleT5bHSp', '...   \n",
      "100005 2018-03-09 14:14:10  {'urls': [{'url': 'https://t.co/3U00mcdonP', '...   \n",
      "100006 2018-03-09 13:01:31  {'urls': [{'url': 'https://t.co/ASleT5bHSp', '...   \n",
      "100007 2018-03-09 12:55:01  {'urls': [{'url': 'https://t.co/3U00mcdonP', '...   \n",
      "100008 2018-03-09 12:48:10  {'urls': [{'url': 'https://t.co/3U00mcdonP', '...   \n",
      "100009 2018-03-09 12:47:10  {'urls': [{'url': 'https://t.co/Biw1szpEif', '...   \n",
      "10001  2018-03-23 00:55:22  {'urls': [{'url': 'https://t.co/VH2ZL8nUMQ', '...   \n",
      "100010 2018-03-09 12:43:34  {'urls': [{'url': 'https://t.co/ASleT5bHSp', '...   \n",
      "100011 2018-03-09 12:40:42  {'urls': [{'url': 'https://t.co/ASleT5bHSp', '...   \n",
      "100012 2018-03-09 11:24:56  {'urls': [{'url': 'https://t.co/3U00mcdonP', '...   \n",
      "100013 2018-03-09 10:55:33  {'urls': [{'url': 'https://t.co/ASleT5bHSp', '...   \n",
      "100014 2018-03-09 10:37:21  {'urls': [{'url': 'https://t.co/WIegWS1ybe', '...   \n",
      "100015 2018-03-09 10:24:31  {'urls': [{'url': 'https://t.co/kKIZsXJw14', '...   \n",
      "100016 2018-03-09 10:02:42  {'urls': [{'url': 'https://t.co/NVCpzLhtd9', '...   \n",
      "100017 2018-03-09 09:59:23  {'urls': [{'url': 'https://t.co/ASleT5bHSp', '...   \n",
      "100018 2018-03-09 09:51:24  {'urls': [{'url': 'https://t.co/ASleT5bHSp', '...   \n",
      "100019 2018-03-09 09:25:32  {'urls': [{'url': 'https://t.co/ASleT5bHSp', '...   \n",
      "10002  2018-03-23 00:39:08  {'urls': [{'url': 'https://t.co/hhXbfHYcRM', '...   \n",
      "100020 2018-03-09 09:21:46  {'urls': [{'url': 'https://t.co/bblG3RurQO', '...   \n",
      "100021 2018-03-09 09:18:32  {'urls': [{'url': 'https://t.co/ASleT5bHSp', '...   \n",
      "...                    ...                                                ...   \n",
      "99972  2018-03-10 04:21:00  {'urls': [{'url': 'https://t.co/rYBAV9Xp3P', '...   \n",
      "99973  2018-03-10 03:22:26  {'urls': [{'url': 'https://t.co/rYBAV9Xp3P', '...   \n",
      "99974  2018-03-10 03:06:53  {'urls': [{'url': 'https://t.co/rYBAV9Xp3P', '...   \n",
      "99975  2018-03-10 02:51:01  {'urls': [{'url': 'https://t.co/UajXQnL0I3', '...   \n",
      "99976  2018-03-10 02:47:50  {'urls': [{'url': 'https://t.co/rYBAV9Xp3P', '...   \n",
      "99977  2018-03-10 00:12:07  {'urls': [{'url': 'https://t.co/nqVnEa75Ku', '...   \n",
      "99978  2018-03-09 22:04:03  {'urls': [{'url': 'https://t.co/nqVnEa75Ku', '...   \n",
      "99979  2018-03-09 21:33:45  {'urls': [{'url': 'https://t.co/nqVnEa75Ku', '...   \n",
      "9998   2018-03-23 01:48:20  {'urls': [{'url': 'https://t.co/NksKwqNA6z', '...   \n",
      "99980  2018-03-09 19:27:03  {'urls': [{'url': 'https://t.co/NpSHs6edKk', '...   \n",
      "99981  2018-03-09 19:05:44  {'urls': [{'url': 'https://t.co/PIVtXr3qTO', '...   \n",
      "99982  2018-03-09 18:56:23  {'urls': [{'url': 'https://t.co/PIVtXr3qTO', '...   \n",
      "99983  2018-03-09 18:53:59  {'urls': [{'url': 'https://t.co/PIVtXr3qTO', '...   \n",
      "99984  2018-03-09 18:36:53  {'urls': [{'url': 'https://t.co/PIVtXr3qTO', '...   \n",
      "99985  2018-03-09 18:34:56  {'urls': [{'url': 'https://t.co/PIVtXr3qTO', '...   \n",
      "99986  2018-03-09 18:33:32  {'urls': [{'url': 'https://t.co/PIVtXr3qTO', '...   \n",
      "99987  2018-03-09 18:19:59  {'urls': [{'url': 'https://t.co/PIVtXr3qTO', '...   \n",
      "99988  2018-03-09 17:49:45  {'urls': [{'url': 'https://t.co/CJvnNU43uK', '...   \n",
      "99989  2018-03-09 17:45:22  {'urls': [{'url': 'https://t.co/PIVtXr3qTO', '...   \n",
      "9999   2018-03-23 01:29:28  {'urls': [{'url': 'https://t.co/jL8ihXfksz', '...   \n",
      "99990  2018-03-09 17:38:54  {'urls': [{'url': 'https://t.co/PIVtXr3qTO', '...   \n",
      "99991  2018-03-09 17:28:23  {'urls': [{'url': 'https://t.co/PIVtXr3qTO', '...   \n",
      "99992  2018-03-09 17:12:12  {'urls': [{'url': 'https://t.co/PIVtXr3qTO', '...   \n",
      "99993  2018-03-09 16:48:02  {'urls': [{'url': 'https://t.co/PIVtXr3qTO', '...   \n",
      "99994  2018-03-09 16:30:21  {'urls': [{'url': 'https://t.co/px7m7E8hSm', '...   \n",
      "99995  2018-03-09 16:26:11  {'urls': [{'url': 'https://t.co/XqWYE2pSkv', '...   \n",
      "99996  2018-03-09 16:23:48  {'urls': [{'url': 'https://t.co/PIVtXr3qTO', '...   \n",
      "99997  2018-03-09 16:14:31  {'urls': [{'url': 'https://t.co/JJRcXuOGX6', '...   \n",
      "99998  2018-03-09 15:48:46  {'urls': [{'url': 'https://t.co/MwIzumSBbQ', '...   \n",
      "99999  2018-03-09 15:36:17  {'urls': [{'url': 'https://t.co/PIVtXr3qTO', '...   \n",
      "\n",
      "        has_media  retweet_count  retweet_median  retweeted    screen_name  \\\n",
      "0           False            889           622.5      False      katyperry   \n",
      "1           False            905           622.5      False      katyperry   \n",
      "10          False            440           622.5      False      katyperry   \n",
      "100         False           1278           622.5      False      katyperry   \n",
      "1000        False            226           180.0      False  KimKardashian   \n",
      "10000       False             67            40.0      False   TheEconomist   \n",
      "100000      False             17             9.0      False   ESPNcricinfo   \n",
      "100001      False              1             9.0      False   ESPNcricinfo   \n",
      "100002      False              5             9.0      False   ESPNcricinfo   \n",
      "100003      False             10             9.0      False   ESPNcricinfo   \n",
      "100004      False              6             9.0      False   ESPNcricinfo   \n",
      "100005      False              7             9.0      False   ESPNcricinfo   \n",
      "100006      False             11             9.0      False   ESPNcricinfo   \n",
      "100007      False             12             9.0      False   ESPNcricinfo   \n",
      "100008      False              3             9.0      False   ESPNcricinfo   \n",
      "100009      False              4             9.0      False   ESPNcricinfo   \n",
      "10001       False             38            40.0      False   TheEconomist   \n",
      "100010      False             10             9.0      False   ESPNcricinfo   \n",
      "100011      False              7             9.0      False   ESPNcricinfo   \n",
      "100012      False              5             9.0      False   ESPNcricinfo   \n",
      "100013      False             10             9.0      False   ESPNcricinfo   \n",
      "100014      False              6             9.0      False   ESPNcricinfo   \n",
      "100015      False              1             9.0      False   ESPNcricinfo   \n",
      "100016      False             13             9.0      False   ESPNcricinfo   \n",
      "100017      False              6             9.0      False   ESPNcricinfo   \n",
      "100018      False              3             9.0      False   ESPNcricinfo   \n",
      "100019      False              8             9.0      False   ESPNcricinfo   \n",
      "10002       False             25            40.0      False   TheEconomist   \n",
      "100020      False              3             9.0      False   ESPNcricinfo   \n",
      "100021      False              4             9.0      False   ESPNcricinfo   \n",
      "...           ...            ...             ...        ...            ...   \n",
      "99972       False              3             9.0      False   ESPNcricinfo   \n",
      "99973       False              9             9.0      False   ESPNcricinfo   \n",
      "99974       False              7             9.0      False   ESPNcricinfo   \n",
      "99975       False             39             9.0      False   ESPNcricinfo   \n",
      "99976       False              4             9.0      False   ESPNcricinfo   \n",
      "99977       False              5             9.0      False   ESPNcricinfo   \n",
      "99978       False              4             9.0      False   ESPNcricinfo   \n",
      "99979       False              3             9.0      False   ESPNcricinfo   \n",
      "9998        False             20            40.0      False   TheEconomist   \n",
      "99980       False             17             9.0      False   ESPNcricinfo   \n",
      "99981       False              5             9.0      False   ESPNcricinfo   \n",
      "99982       False              7             9.0      False   ESPNcricinfo   \n",
      "99983       False              3             9.0      False   ESPNcricinfo   \n",
      "99984       False              7             9.0      False   ESPNcricinfo   \n",
      "99985       False              3             9.0      False   ESPNcricinfo   \n",
      "99986       False              2             9.0      False   ESPNcricinfo   \n",
      "99987       False              4             9.0      False   ESPNcricinfo   \n",
      "99988       False              8             9.0      False   ESPNcricinfo   \n",
      "99989       False              4             9.0      False   ESPNcricinfo   \n",
      "9999        False             27            40.0      False   TheEconomist   \n",
      "99990       False              3             9.0      False   ESPNcricinfo   \n",
      "99991       False              3             9.0      False   ESPNcricinfo   \n",
      "99992       False              7             9.0      False   ESPNcricinfo   \n",
      "99993       False              5             9.0      False   ESPNcricinfo   \n",
      "99994       False              8             9.0      False   ESPNcricinfo   \n",
      "99995       False             19             9.0      False   ESPNcricinfo   \n",
      "99996       False              5             9.0      False   ESPNcricinfo   \n",
      "99997       False             15             9.0      False   ESPNcricinfo   \n",
      "99998       False             24             9.0      False   ESPNcricinfo   \n",
      "99999       False              6             9.0      False   ESPNcricinfo   \n",
      "\n",
      "                                                     text           time  \n",
      "0       Zomg this is me everyday after I read the whol...  1523074012000  \n",
      "1                         V Cute! https://t.co/Dqgw5v9a13  1523073260000  \n",
      "10      Out of the ashes, rises the phoenix @AdaVox #a...  1522728136000  \n",
      "100     I thought it was an exboyfriend, thank God it ...  1521519286000  \n",
      "1000    Alright I’m here watching KUWTK Finale with yo...  1520217343000  \n",
      "10000   Slapping tariffs on over 10% of Chinese goods ...  1521767456000  \n",
      "100000  Kagiso Rabada takes five and then scores an un...  1520609567000  \n",
      "100001  @prashantc841 Yes, two quarter-finals were pla...  1520609334000  \n",
      "100002  Pat Cummins strikes in his first over! Aiden M...  1520608369000  \n",
      "100003  JSW Sports buys 50% stake in Delhi Daredevils ...  1520607757000  \n",
      "100004  Australia bowled out for 243. Is that a decent...  1520605679000  \n",
      "100005  Brendon McCullum dropped: all signs pointing t...  1520604850000  \n",
      "100006  Rabada on a hat-trick! Cummins gone for a gold...  1520600491000  \n",
      "100007  17-year-old Shaheen Afridi has three wickets i...  1520600101000  \n",
      "100008  Sunil Narine 4-0-16-2. Can he help @lahoreqala...  1520599690000  \n",
      "100009  #PSL newfile: Mailk's 300, red-hot Ronchi \\n\\n...  1520599630000  \n",
      "10001   Technological advancement means that humans un...  1521766522000  \n",
      "100010  Ten minutes ago Australia were 161 for 3, with...  1520599414000  \n",
      "100011  Stunning spell from Rabada before tea: Smith a...  1520599242000  \n",
      "100012  Brendon McCullum continues to lead @lahoreqala...  1520594696000  \n",
      "100013  A second strike for Philander! \\n\\nUsman Khawa...  1520592933000  \n",
      "100014  'You can't expect such a performance from Afgh...  1520591841000  \n",
      "100015  High-flying England prepared for turbulence \\n...  1520591071000  \n",
      "100016  Schooling coaches, family BBQs and debut anger...  1520589762000  \n",
      "100017  Vernon Philander gets the breakthrough for Sou...  1520589563000  \n",
      "100018  David Warner brings up his seventh 50-plus sco...  1520589084000  \n",
      "100019  14 overs till drinks: 23 runs\\n\\nFour overs si...  1520587532000  \n",
      "10002   Seven years after a series of revolutions led ...  1521765548000  \n",
      "100020  All eyes on Ross Taylor ahead of the #NZvENG s...  1520587306000  \n",
      "100021  David Warner edges off Kagiso Rabada, but ther...  1520587112000  \n",
      "...                                                   ...            ...  \n",
      "99972   England close in on victory.\\n\\nWho's your pic...  1520655660000  \n",
      "99973   Jonny Bairstow slams Ish Sodhi for five sixes ...  1520652146000  \n",
      "99974   50 comes up off 38 balls for Jonny Bairstow as...  1520651213000  \n",
      "99975   How much is an IPL franchise worth? Delhi Dare...  1520650261000  \n",
      "99976   England openers bring up the 50 stand. One-way...  1520650070000  \n",
      "99977   Nicholls and Santner, New Zealand's last pair ...  1520640727000  \n",
      "99978   Colin Munro's series:\\n6\\n1\\n49\\n0\\n0\\n\\nhttps...  1520633043000  \n",
      "99979   More injury news: Jason Roy ruled out with a b...  1520631225000  \n",
      "9998    If AT&amp;T wins the case against the Justice ...  1521769700000  \n",
      "99980   Red-hot Rabada needs to cool off \\n\\nhttps://t...  1520623623000  \n",
      "99981   The slide continues! \\n\\n92 for 8 \\n\\n https:/...  1520622344000  \n",
      "99982   A beauty from Zafar Gohar and Darren Sammy did...  1520621783000  \n",
      "99983   Peshawar Zalmi crumbling against spin! \\n\\n76 ...  1520621639000  \n",
      "99984   Two in two! Samit Patel on 🔥 \\n\\nDwayne Smith ...  1520620613000  \n",
      "99985   Samit Patel strikes again! Andre Fletcher gone...  1520620496000  \n",
      "99986   The review saves Dwayne Smith, who was given o...  1520620412000  \n",
      "99987   Samit Patel sets off on a celebratory run afte...  1520619599000  \n",
      "99988   Keemo Paul replaces injured Sheldon Cottrell i...  1520617785000  \n",
      "99989   JP Duminy and Asif Ali steer Islamabad United ...  1520617522000  \n",
      "9999    E.ON expects €600m-800m a year of savings from...  1521768568000  \n",
      "99990   A searing yorker from Wahab Riaz bowls Asif Al...  1520617134000  \n",
      "99991   150 up for Islamabad United in 17.1 overs\\n\\nh...  1520616503000  \n",
      "99992   Fifty for JP Duminy! \\n\\nhttps://t.co/PIVtXr3q...  1520615532000  \n",
      "99993   After 10 overs, Islamabad United are 78 for 1....  1520614082000  \n",
      "99994   Teams brace for winner-takes-it-all contest \\n...  1520613021000  \n",
      "99995   Kagiso Rabada took 5 for 13 in the space of 18...  1520612771000  \n",
      "99996   Luke Ronchi hits three consecutive fours off W...  1520612628000  \n",
      "99997   Everything you need to know about the Pakistan...  1520612071000  \n",
      "99998   How costly will Kagiso Rabada's shoulder brush...  1520610526000  \n",
      "99999   Darren Sammy is back for Peshawar Zalmi, and o...  1520609777000  \n",
      "\n",
      "[118604 rows x 9 columns]\n",
      "43793\n"
     ]
    }
   ],
   "source": [
    "train=pd.read_json(\"train_original_tweets_top1000full_text_only.json\")\n",
    "test=pd.read_json(\"test_original_tweets_top1000full_text_only.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = \"Wed Apr 15 00:00:00 +0000 2018\"\n",
    "start_time = datetime.strptime(start_time, \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "\n",
    "test=test[test[\"created_at\"]<start_time]=test[test[\"created_at\"]<start_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118604\n",
      "20157\n",
      "0.8547358407621738\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(test))\n",
    "print(len(train)/(len(test)+len(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(texts):\n",
    "    cleaned_text = []\n",
    "    for text in texts:\n",
    "        # remove &quot and &amp\n",
    "        text = re.sub(r'&quot;(.*?)&quot;', \"\\g<1>\", text)\n",
    "        text = re.sub(r'&amp;', \"\", text)\n",
    "\n",
    "        # replace emoticon\n",
    "        text = re.sub(r'(^| )(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)', \"\\g<1>TOKEMOTICON\", text)\n",
    "\n",
    "        text = text.lower()\n",
    "        text = text.replace(\"tokemoticon\", \"TOKEMOTICON\")\n",
    "\n",
    "        # replace url\n",
    "        text = re.sub(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?',\n",
    "                    \"TOKURL\", text)\n",
    "\n",
    "        # replace mention\n",
    "        text = re.sub(r'@[\\w]+', \"TOKMENTION\", text)\n",
    "\n",
    "        # replace hashtag\n",
    "        text = re.sub(r'#[\\w]+', \"TOKHASHTAG\", text)\n",
    "\n",
    "        # replace dollar\n",
    "        text = re.sub(r'\\$\\d+', \"TOKDOLLAR\", text)\n",
    "\n",
    "        \n",
    "        # remove multiple spaces\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "\n",
    "        # remove newline\n",
    "        text = re.sub(r'\\n', ' ', text)\n",
    "\n",
    "        cleaned_text.append(text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def get_average_word_length(words,english_punctuations):\n",
    "    word_count=0\n",
    "    word_total_length=0\n",
    "    for word in words:\n",
    "        if word not in english_punctuations:\n",
    "            word_total_length+=len(word)\n",
    "            word_count+=1\n",
    "    return word_total_length/word_count\n",
    "\n",
    "def generate_common_features(train_df):\n",
    "    english_punctuations = [',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%']\n",
    "    train_df[\"tokenized_words\"]=train_df[\"text\"].apply(lambda x:word_tokenize(x))\n",
    "    train_df[\"hashtag_num\"]=train_df[\"entities\"].apply(lambda x:len(x[\"hashtags\"]))\n",
    "    train_df[\"link_num\"]=train_df[\"entities\"].apply(lambda x:len(x[\"urls\"]))\n",
    "    train_df[\"mention_num\"]=train_df[\"entities\"].apply(lambda x:len(x[\"user_mentions\"]))\n",
    "    train_df[\"time_of_date\"]=train_df[\"created_at\"].apply(lambda x:x.hour*24+x.minute)\n",
    "    train_df[\"len_of_tweet\"]=train_df[\"text\"].apply(lambda x:len(x))\n",
    "    train_df[\"avg_len_of_word\"]=train_df[\"tokenized_words\"].apply(lambda x:get_average_word_length(x,english_punctuations))\n",
    "    train_df[\"word_count\"]=train_df[\"tokenized_words\"].apply(lambda x:len(x))\n",
    "    \n",
    "    \n",
    "#4 features involving most tweetable words\n",
    "most_tweetable_words={\"you\":0,\"twitter\":1,\"please\":2,\"retweet\":3,\"post\":4,\"blog\":5,\"social\":6,\n",
    "                     \"free\":7,\"media\":8,\"help\":9,\"great\":10,\"10\":11,\"follow\":12,\"how\":13,\n",
    "                      \"to\":14,\"top\":15,\"check\":16,\"out\":17,\"this\":18,\"people\":19,\"vote\":20,\n",
    "                     \"?\":21}\n",
    "def add_most_tweetable_words_features(pd):\n",
    "    pd[\"aggreg_exist\"]=[1 if any(t in most_tweetable_words for t in i.split()) else 0 for i in pd[\"text\"]]\n",
    "    pd[\"aggreg_tf\"]=[sum([1 if t in most_tweetable_words else 0 for t in i.split()]) for i in pd[\"text\"]]\n",
    "    for i in most_tweetable_words:\n",
    "        pd[i+\"_exist\"]=[1 if any(t==i for t in j.split()) else 0 for j in pd[\"text\"]]\n",
    "        pd[i+\"_tf\"]=[sum([1 if t==i else 0 for t in j.split()]) for j in pd[\"text\"]]\n",
    "\n",
    "def add_question_features(pd):\n",
    "    pd[\"ques_exist\"]=[1 if collections.Counter(i)[\"?\"]!=0 else 0 for i in pd[\"text\"]]\n",
    "    pd[\"ques_exist\"]=[collections.Counter(i)[\"?\"] for i in pd[\"text\"]]\n",
    "\n",
    "def add_pn_word_features(pd):\n",
    "    pd[\"positive_word\"]=[sum([1 if t in positive_words else 0 for t in j.split()]) for j in pd[\"text\"]]\n",
    "    pd[\"negative_word\"]=[sum([1 if t in negative_words else 0 for t in j.split()]) for j in pd[\"text\"]]\n",
    "    \n",
    "\n",
    "f=open(\"positive_words.txt\")\n",
    "positive_words=set()\n",
    "for line in f.readlines():\n",
    "    positive_words.add(line.strip())\n",
    "f.close()\n",
    "\n",
    "f=open(\"negative_words.txt\")\n",
    "negative_words=set()\n",
    "for line in f.readlines():\n",
    "    negative_words.add(line.strip())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"text\"]=cleanup_text(train[\"text\"].values)\n",
    "test[\"text\"]=cleanup_text(test[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfile = 'glove.twitter.27B/glove.twitter.27B.100d.txt' # word vector file, can be downloaded from GloVe website\n",
    "weightfile = 'enwiki_vocab_min200.txt' # each line is a word and its frequency\n",
    "weightpara = 1e-3 # the parameter in the SIF weighting scheme, usually in the range [3e-5, 3e-3]\n",
    "rmpc = 1 # number of principal components to remove in SIF weighting scheme\n",
    "# load word vectors\n",
    "(words, We) = data_io.getWordmap(wordfile)\n",
    "# load word weights\n",
    "word2weight = data_io.getWordWeight(weightfile, weightpara) # word2weight['str'] is the weight for the word 'str'\n",
    "weight4ind = data_io.getWeight(words, word2weight) # weight4ind[i] is the weight for the i-th word\n",
    "# load sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_SIF_embedding_vectors(df,weight4ind,words,We,rmpc):\n",
    "    x, m = data_io.sentences2idx(df[\"text\"].values, words) # x is the array of word indices, m is the binary mask indicating whether there is a word in that location\n",
    "    w = data_io.seq2weight(x, m, weight4ind) # get word weights\n",
    "    embedding = SIF_embedding.SIF_embedding(We, x, w, rmpc)\n",
    "    embed_list=list(map(lambda x:x,embedding))\n",
    "    return embed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"embed\"]=generate_SIF_embedding_vectors(train,weight4ind,words,We,rmpc)\n",
    "test[\"embed\"]=generate_SIF_embedding_vectors(test,weight4ind,words,We,rmpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"greater_times\"]=train[\"retweet_count\"]/train[\"retweet_median\"]\n",
    "popular_train_df=train[train[\"greater_times\"]>=10]\n",
    "unpopular_train_df=train[train[\"greater_times\"]<=0.5]\n",
    "hot_vectors=np.array([x for x in popular_train_df[\"embed\"].values])\n",
    "cold_vectors=np.array([x for x in unpopular_train_df[\"embed\"].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "KM = kmeans.KMeans()\n",
    "y,centers_pop=KM.train(hot_vectors,KM.init_centers(hot_vectors,k),niters=300)\n",
    "KM = kmeans.KMeans()\n",
    "y,centers_unpop=KM.train(cold_vectors,KM.init_centers(cold_vectors,k),niters=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):    \n",
    "    train[\"pop_dist_\"+str(i)]=train[\"embed\"].apply(lambda x:np.linalg.norm(x-centers_pop[i]))\n",
    "    train[\"uppop_dist_\"+str(i)]=train[\"embed\"].apply(lambda x:np.linalg.norm(x-centers_unpop[i]))\n",
    "    test[\"pop_dist_\"+str(i)]=test[\"embed\"].apply(lambda x:np.linalg.norm(x-centers_pop[i]))\n",
    "    test[\"uppop_dist_\"+str(i)]=test[\"embed\"].apply(lambda x:np.linalg.norm(x-centers_unpop[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_common_features(train)\n",
    "generate_common_features(test)\n",
    "\n",
    "add_most_tweetable_words_features(train)\n",
    "add_question_features(train)\n",
    "add_most_tweetable_words_features(test)\n",
    "add_question_features(test)\n",
    "add_pn_word_features(train)\n",
    "add_pn_word_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train[train[\"word_count\"]>=3]\n",
    "test=test[test[\"word_count\"]>=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luxiaowei/anaconda2/envs/python35/lib/python3.5/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "lang_model = natural_language_processing.LanguageModel(popular_train_df[\"text\"].values, 3)\n",
    "train[\"perplexity\"]=train[\"text\"].apply(\n",
    "    lambda x:lang_model.perplexity(x))\n",
    "\n",
    "test[\"perplexity\"]=test[\"text\"].apply(\n",
    "    lambda x:lang_model.perplexity(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train.copy()\n",
    "test_data=test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_target=train_data[\"retweet_median\"]\n",
    "train_data[\"diff\"]=train_data[\"retweet_count\"]/train_data[\"retweet_median\"]\n",
    "train_data[\"target\"]=[1 if i>=2 else 0 for i in train_data[\"diff\"]]\n",
    "\n",
    "test_data[\"diff\"]=test_data[\"retweet_count\"]/test_data[\"retweet_median\"]\n",
    "test_data[\"target\"]=[1 if i>=2 else 0 for i in test_data[\"diff\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns=['perplexity','word_count', 'hashtag_num', 'link_num',\n",
    "       'mention_num', 'time_of_date', 'len_of_tweet', 'avg_len_of_word',\n",
    "        'twitter_tf','this_tf', 'to_tf','how_tf','great_tf', 'check_tf',\n",
    "       'retweet_tf', 'free_tf','blog_tf', '?_tf', 'help_tf', 'post_tf',\n",
    "        'top_tf','please_tf','out_tf', 'social_tf','10_tf',\n",
    "        'follow_tf', 'you_tf',\n",
    "       'vote_tf','people_tf', 'media_tf', 'pop_dist_0', 'uppop_dist_0', 'pop_dist_1',\n",
    "       'uppop_dist_1', 'pop_dist_2', 'uppop_dist_2', 'pop_dist_3',\n",
    "       'uppop_dist_3', 'pop_dist_4', 'uppop_dist_4','positive_word',\"negative_word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuract 0.7615900621118012\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100)  \n",
    "RandomForestModel= clf.fit(train_data[feature_columns],train_data[\"target\"])\n",
    "r = clf.score(test_data[feature_columns], test_data[\"target\"])  \n",
    "print(\"accuract\",r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            feature  importance\n",
      "16          blog_tf    0.000053\n",
      "17             ?_tf    0.000391\n",
      "14       retweet_tf    0.000401\n",
      "25        follow_tf    0.000416\n",
      "19          post_tf    0.000618\n",
      "27          vote_tf    0.000623\n",
      "15          free_tf    0.000664\n",
      "23        social_tf    0.000668\n",
      "13         check_tf    0.000682\n",
      "29         media_tf    0.000752\n",
      "20           top_tf    0.000871\n",
      "8        twitter_tf    0.000891\n",
      "24            10_tf    0.000976\n",
      "21        please_tf    0.001030\n",
      "18          help_tf    0.001505\n",
      "12         great_tf    0.001630\n",
      "28        people_tf    0.002464\n",
      "11           how_tf    0.002596\n",
      "22           out_tf    0.002806\n",
      "9           this_tf    0.004934\n",
      "26           you_tf    0.005945\n",
      "2       hashtag_num    0.008741\n",
      "41    negative_word    0.010483\n",
      "10            to_tf    0.010546\n",
      "3          link_num    0.012267\n",
      "4       mention_num    0.013717\n",
      "40    positive_word    0.014167\n",
      "1        word_count    0.040974\n",
      "38       pop_dist_4    0.047571\n",
      "37     uppop_dist_3    0.047652\n",
      "35     uppop_dist_2    0.047815\n",
      "36       pop_dist_3    0.048827\n",
      "33     uppop_dist_1    0.048906\n",
      "30       pop_dist_0    0.049100\n",
      "34       pop_dist_2    0.049923\n",
      "39     uppop_dist_4    0.050109\n",
      "31     uppop_dist_0    0.052360\n",
      "6      len_of_tweet    0.053247\n",
      "32       pop_dist_1    0.053960\n",
      "7   avg_len_of_word    0.055339\n",
      "5      time_of_date    0.073244\n",
      "0        perplexity    0.180133\n"
     ]
    }
   ],
   "source": [
    "feature_importance=pd.DataFrame({\"importance\":clf.feature_importances_,\"feature\":train_data[feature_columns].columns})\n",
    "print(feature_importance.sort_values(by=\"importance\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

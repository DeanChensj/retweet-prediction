{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from collections import defaultdict\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from datetime import datetime\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_json(\"train_original_tweets_top1000.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_json(\"test_original_tweets_top1000.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>entities</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweet_median</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-04-09 18:04:33</td>\n",
       "      <td>{'symbols': [], 'hashtags': [], 'user_mentions...</td>\n",
       "      <td>1230</td>\n",
       "      <td>899.0</td>\n",
       "      <td>False</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>I can't wait for you to üëÅwitnessüëÅ my baby bird...</td>\n",
       "      <td>1523297073000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-04-09 02:57:37</td>\n",
       "      <td>{'symbols': [], 'hashtags': [{'text': 'america...</td>\n",
       "      <td>1478</td>\n",
       "      <td>899.0</td>\n",
       "      <td>False</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>Who's ready for #americanidol ‚ÅâÔ∏èüôãüèºLive on Inst...</td>\n",
       "      <td>1523242657000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-04-03 04:27:30</td>\n",
       "      <td>{'symbols': [], 'hashtags': [{'text': 'america...</td>\n",
       "      <td>411</td>\n",
       "      <td>899.0</td>\n",
       "      <td>False</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>üëÇüèª@calebleemusic WE ARE LISTENING üëÇüèªAND WE LIK...</td>\n",
       "      <td>1522729650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2018-03-23 17:52:04</td>\n",
       "      <td>{'symbols': [], 'hashtags': [], 'user_mentions...</td>\n",
       "      <td>1016</td>\n",
       "      <td>899.0</td>\n",
       "      <td>False</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>üö®‚ùóTHIS JUST IN‚ùóüö®\\nNow your little one can make...</td>\n",
       "      <td>1521827524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>2018-01-29 20:06:42</td>\n",
       "      <td>{'symbols': [], 'hashtags': [], 'user_mentions...</td>\n",
       "      <td>7339</td>\n",
       "      <td>6050.5</td>\n",
       "      <td>False</td>\n",
       "      <td>ladygaga</td>\n",
       "      <td>Here‚Äôs the Piano Version of ‚ÄúJoanne‚Äù music vid...</td>\n",
       "      <td>1517256402000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              created_at                                           entities  \\\n",
       "0    2018-04-09 18:04:33  {'symbols': [], 'hashtags': [], 'user_mentions...   \n",
       "1    2018-04-09 02:57:37  {'symbols': [], 'hashtags': [{'text': 'america...   \n",
       "10   2018-04-03 04:27:30  {'symbols': [], 'hashtags': [{'text': 'america...   \n",
       "100  2018-03-23 17:52:04  {'symbols': [], 'hashtags': [], 'user_mentions...   \n",
       "1000 2018-01-29 20:06:42  {'symbols': [], 'hashtags': [], 'user_mentions...   \n",
       "\n",
       "      retweet_count  retweet_median  retweeted screen_name  \\\n",
       "0              1230           899.0      False   katyperry   \n",
       "1              1478           899.0      False   katyperry   \n",
       "10              411           899.0      False   katyperry   \n",
       "100            1016           899.0      False   katyperry   \n",
       "1000           7339          6050.5      False    ladygaga   \n",
       "\n",
       "                                                   text           time  \n",
       "0     I can't wait for you to üëÅwitnessüëÅ my baby bird...  1523297073000  \n",
       "1     Who's ready for #americanidol ‚ÅâÔ∏èüôãüèºLive on Inst...  1523242657000  \n",
       "10    üëÇüèª@calebleemusic WE ARE LISTENING üëÇüèªAND WE LIK...  1522729650000  \n",
       "100   üö®‚ùóTHIS JUST IN‚ùóüö®\\nNow your little one can make...  1521827524000  \n",
       "1000  Here‚Äôs the Piano Version of ‚ÄúJoanne‚Äù music vid...  1517256402000  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295720"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>entities</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweet_median</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-04-16 23:48:44</td>\n",
       "      <td>{'symbols': [], 'hashtags': [{'text': 'America...</td>\n",
       "      <td>1320</td>\n",
       "      <td>899.0</td>\n",
       "      <td>False</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>I have good news and bad news today - ‚òπÔ∏è: I wo...</td>\n",
       "      <td>1523922524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-04-15 23:38:20</td>\n",
       "      <td>{'symbols': [], 'hashtags': [{'text': 'america...</td>\n",
       "      <td>1559</td>\n",
       "      <td>899.0</td>\n",
       "      <td>False</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>Ooh, almost time for another #americanidol! üëèüèº...</td>\n",
       "      <td>1523835500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-04-10 18:49:07</td>\n",
       "      <td>{'symbols': [], 'hashtags': [], 'user_mentions...</td>\n",
       "      <td>5770</td>\n",
       "      <td>43399.0</td>\n",
       "      <td>True</td>\n",
       "      <td>justinbieber</td>\n",
       "      <td>RT @poobear: Thanks for this @HannahStocking! ...</td>\n",
       "      <td>1523386147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2018-04-16 14:04:03</td>\n",
       "      <td>{'symbols': [], 'hashtags': [], 'user_mentions...</td>\n",
       "      <td>81</td>\n",
       "      <td>111.0</td>\n",
       "      <td>False</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>Monday mood. https://t.co/IFNJryvUcE</td>\n",
       "      <td>1523887443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>2018-04-12 23:17:04</td>\n",
       "      <td>{'symbols': [], 'hashtags': [], 'user_mentions...</td>\n",
       "      <td>96</td>\n",
       "      <td>104.0</td>\n",
       "      <td>False</td>\n",
       "      <td>nytimes</td>\n",
       "      <td>For a nuanced picture of Lebanon and its peopl...</td>\n",
       "      <td>1523575024000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              created_at                                           entities  \\\n",
       "0    2018-04-16 23:48:44  {'symbols': [], 'hashtags': [{'text': 'America...   \n",
       "1    2018-04-15 23:38:20  {'symbols': [], 'hashtags': [{'text': 'america...   \n",
       "10   2018-04-10 18:49:07  {'symbols': [], 'hashtags': [], 'user_mentions...   \n",
       "100  2018-04-16 14:04:03  {'symbols': [], 'hashtags': [], 'user_mentions...   \n",
       "1000 2018-04-12 23:17:04  {'symbols': [], 'hashtags': [], 'user_mentions...   \n",
       "\n",
       "      retweet_count  retweet_median  retweeted   screen_name  \\\n",
       "0              1320           899.0      False     katyperry   \n",
       "1              1559           899.0      False     katyperry   \n",
       "10             5770         43399.0       True  justinbieber   \n",
       "100              81           111.0      False       YouTube   \n",
       "1000             96           104.0      False       nytimes   \n",
       "\n",
       "                                                   text           time  \n",
       "0     I have good news and bad news today - ‚òπÔ∏è: I wo...  1523922524000  \n",
       "1     Ooh, almost time for another #americanidol! üëèüèº...  1523835500000  \n",
       "10    RT @poobear: Thanks for this @HannahStocking! ...  1523386147000  \n",
       "100                Monday mood. https://t.co/IFNJryvUcE  1523887443000  \n",
       "1000  For a nuanced picture of Lebanon and its peopl...  1523575024000  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80218"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word_length(words,english_punctuations):\n",
    "    word_count=0\n",
    "    word_total_length=0\n",
    "    for word in words:\n",
    "        if word not in english_punctuations:\n",
    "            word_total_length+=len(word)\n",
    "            word_count+=1\n",
    "    return word_total_length/word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_common_features(train_df):\n",
    "    english_punctuations = [',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%']\n",
    "    train_df[\"tokenized_words\"]=train_df[\"text\"].apply(lambda x:word_tokenize(x))\n",
    "    train_df[\"hashtag_num\"]=train_df[\"entities\"].apply(lambda x:len(x[\"hashtags\"]))\n",
    "    train_df[\"link_num\"]=train_df[\"entities\"].apply(lambda x:len(x[\"urls\"]))\n",
    "    train_df[\"mention_num\"]=train_df[\"entities\"].apply(lambda x:len(x[\"user_mentions\"]))\n",
    "    train_df[\"time_of_date\"]=train_df[\"created_at\"].apply(lambda x:x.hour*24+x.minute)\n",
    "    train_df[\"len_of_tweet\"]=train_df[\"text\"].apply(lambda x:len(x))\n",
    "    train_df[\"avg_len_of_word\"]=train_df[\"tokenized_words\"].apply(lambda x:get_average_word_length(x,english_punctuations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict={}\n",
    "temp_count=0\n",
    "for i in train_data[\"screen_name\"]:\n",
    "    if i in name_dict:\n",
    "        continue\n",
    "    else:\n",
    "        name_dict[i]=temp_count\n",
    "        temp_count+=1\n",
    "\n",
    "for i in test_data[\"screen_name\"]:\n",
    "    if i in name_dict:\n",
    "        continue\n",
    "    else:\n",
    "        name_dict[i]=temp_count\n",
    "        temp_count+=1\n",
    "        \n",
    "train_data[\"screen_name\"]=[name_dict[i] for i in train_data[\"screen_name\"]]\n",
    "test_data[\"screen_name\"]=[name_dict[i] for i in test_data[\"screen_name\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295720"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'symbols': [], 'hashtags': [{'text': 'AmericanIdol', 'indices': [73, 86]}], 'user_mentions': [], 'urls': [{'display_url': 'twitter.com/i/web/status/9‚Ä¶', 'expanded_url': 'https://twitter.com/i/web/status/986028650607734784', 'indices': [117, 140], 'url': 'https://t.co/VSMnfhh1EG'}]}\n"
     ]
    }
   ],
   "source": [
    "for i in test_data[\"entities\"].head():\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"hashtags_num\"]=[len(i['hashtags']) for i in test_data[\"entities\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"hashtags_num\"]=[len(i['hashtags']) for i in train_data[\"entities\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"user_mentions\"]=[len(i['user_mentions']) for i in train_data[\"entities\"]]\n",
    "test_data[\"user_mentions\"]=[len(i['user_mentions']) for i in test_data[\"entities\"]]\n",
    "train_data[\"urls\"]=[len(i['urls']) for i in train_data[\"entities\"]]\n",
    "test_data[\"urls\"]=[len(i['urls']) for i in test_data[\"entities\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"hashtags_num\"]=[len(i['hashtags']) for i in train_data[\"entities\"]]\n",
    "test_data[\"hashtags_num\"]=[len(i['hashtags']) for i in test_data[\"entities\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "labels ['entities'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-5b175fe6469f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/python35/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   2528\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2530\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/python35/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   2560\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2563\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/python35/lib/python3.5/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   3742\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3743\u001b[0m                 raise ValueError('labels %s not contained in axis' %\n\u001b[0;32m-> 3744\u001b[0;31m                                  labels[mask])\n\u001b[0m\u001b[1;32m   3745\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3746\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: labels ['entities'] not contained in axis"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 features involving most tweetable words\n",
    "most_tweetable_words={\"you\":0,\"twitter\":1,\"please\":2,\"retweet\":3,\"post\":4,\"blog\":5,\"social\":6,\n",
    "                     \"free\":7,\"media\":8,\"help\":9,\"great\":10,\"10\":11,\"follow\":12,\"how\":13,\n",
    "                      \"to\":14,\"top\":15,\"check\":16,\"out\":17,\"this\":18,\"people\":19,\"votr\":20,\n",
    "                     \"?\":21}\n",
    "def add_most_tweetable_words_features(pd):\n",
    "    pd[\"aggreg_exist\"]=[1 if any(t in most_tweetable_words for t in i.split()) else 0 for i in pd[\"text\"]]\n",
    "    pd[\"aggreg_tf\"]=[sum([1 if t in most_tweetable_words else 0 for t in i.split()]) for i in pd[\"text\"]]\n",
    "    for i in most_tweetable_words:\n",
    "        pd[i+\"_exist\"]=[1 if any(t==i for t in j.split()) else 0 for j in pd[\"text\"]]\n",
    "        pd[i+\"_tf\"]=[sum([1 if t==i else 0 for t in j.split()]) for j in pd[\"text\"]]\n",
    "\n",
    "def add_question_features(pd):\n",
    "    pd[\"ques_exist\"]=[1 if collections.Counter(i)[\"?\"]!=0 else 0 for i in pd[\"text\"]]\n",
    "    pd[\"ques_exist\"]=[collections.Counter(i)[\"?\"] for i in pd[\"text\"]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'entities'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda2/envs/python35/lib/python3.5/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2524\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2525\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'entities'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-235-df3f40de9ca7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_common_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgenerate_common_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-234-d0d093a3f3b7>\u001b[0m in \u001b[0;36mgenerate_common_features\u001b[0;34m(train_df)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0menglish_punctuations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'('\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m')'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'['\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m']'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'&'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'!'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'@'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokenized_words\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hashtag_num\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hashtags\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"link_num\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"urls\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mention_num\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user_mentions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/python35/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/python35/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/python35/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1842\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/python35/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3842\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3843\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3844\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3845\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/python35/lib/python3.5/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2525\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2527\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'entities'"
     ]
    }
   ],
   "source": [
    "generate_common_features(train_data)\n",
    "generate_common_features(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=test_data.drop([\"entities\"],axis=1)\n",
    "train_data=train_data.drop([\"entities\"],axis=1)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              created_at  retweet_count  retweet_median  retweeted  \\\n",
      "0    2018-04-09 18:04:33           1230           899.0      False   \n",
      "1    2018-04-09 02:57:37           1478           899.0      False   \n",
      "10   2018-04-03 04:27:30            411           899.0      False   \n",
      "100  2018-03-23 17:52:04           1016           899.0      False   \n",
      "1000 2018-01-29 20:06:42           7339          6050.5      False   \n",
      "\n",
      "      screen_name                                               text  \\\n",
      "0               0  I can't wait for you to üëÅwitnessüëÅ my baby bird...   \n",
      "1               0  Who's ready for #americanidol ‚ÅâÔ∏èüôãüèºLive on Inst...   \n",
      "10              0  üëÇüèª@calebleemusic WE ARE LISTENING üëÇüèªAND WE LIK...   \n",
      "100             0  üö®‚ùóTHIS JUST IN‚ùóüö®\\nNow your little one can make...   \n",
      "1000            1  Here‚Äôs the Piano Version of ‚ÄúJoanne‚Äù music vid...   \n",
      "\n",
      "               time  hashtags_num  user_mentions  urls     ...      people_tf  \\\n",
      "0     1523297073000             0              1     1     ...              0   \n",
      "1     1523242657000             1              0     1     ...              0   \n",
      "10    1522729650000             1              1     0     ...              0   \n",
      "100   1521827524000             0              1     1     ...              0   \n",
      "1000  1517256402000             0              0     1     ...              0   \n",
      "\n",
      "      retweet_exist  retweet_tf  10_exist  10_tf  you_exist  you_tf  \\\n",
      "0                 0           0         0      0          1       1   \n",
      "1                 0           0         0      0          0       0   \n",
      "10                0           0         0      0          0       0   \n",
      "100               0           0         0      0          0       0   \n",
      "1000              0           0         0      0          0       0   \n",
      "\n",
      "      top_exist  top_tf  ques_exist  \n",
      "0             0       0           0  \n",
      "1             0       0           0  \n",
      "10            0       0           0  \n",
      "100           0       0           0  \n",
      "1000          0       0           0  \n",
      "\n",
      "[5 rows x 57 columns]\n"
     ]
    }
   ],
   "source": [
    "add_most_tweetable_words_features(train_data)\n",
    "add_question_features(train_data)\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295720"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              created_at  retweet_count  retweet_median  retweeted  \\\n",
      "0    2018-04-16 23:48:44           1320           899.0      False   \n",
      "1    2018-04-15 23:38:20           1559           899.0      False   \n",
      "10   2018-04-10 18:49:07           5770         43399.0       True   \n",
      "100  2018-04-16 14:04:03             81           111.0      False   \n",
      "1000 2018-04-12 23:17:04             96           104.0      False   \n",
      "\n",
      "      screen_name                                               text  \\\n",
      "0               0  I have good news and bad news today - ‚òπÔ∏è: I wo...   \n",
      "1               0  Ooh, almost time for another #americanidol! üëèüèº...   \n",
      "10            732  RT @poobear: Thanks for this @HannahStocking! ...   \n",
      "100           228               Monday mood. https://t.co/IFNJryvUcE   \n",
      "1000          795  For a nuanced picture of Lebanon and its peopl...   \n",
      "\n",
      "               time  hashtags_num  user_mentions  urls     ...      people_tf  \\\n",
      "0     1523922524000             1              0     1     ...              0   \n",
      "1     1523835500000             1              0     1     ...              0   \n",
      "10    1523386147000             0              2     1     ...              0   \n",
      "100   1523887443000             0              0     1     ...              0   \n",
      "1000  1523575024000             0              0     1     ...              0   \n",
      "\n",
      "      retweet_exist  retweet_tf  10_exist  10_tf  you_exist  you_tf  \\\n",
      "0                 0           0         0      0          0       0   \n",
      "1                 0           0         0      0          0       0   \n",
      "10                0           0         0      0          0       0   \n",
      "100               0           0         0      0          0       0   \n",
      "1000              0           0         0      0          0       0   \n",
      "\n",
      "      top_exist  top_tf  ques_exist  \n",
      "0             0       0           0  \n",
      "1             0       0           0  \n",
      "10            0       0           0  \n",
      "100           0       0           0  \n",
      "1000          0       0           0  \n",
      "\n",
      "[5 rows x 57 columns]\n"
     ]
    }
   ],
   "source": [
    "add_most_tweetable_words_features(test_data)\n",
    "add_question_features(test_data)\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_target=train_data[\"retweet_median\"]\n",
    "train_target[\"diff\"]=train_data[\"retweet_count\"]-train_data[\"retweet_median\"]\n",
    "train_target[\"label\"]=[1 if i>=0 else 0 for i in train_target[\"diff\"]]\n",
    "\n",
    "test_target[\"diff\"]=test_data[\"retweet_count\"]-test_data[\"retweet_median\"]\n",
    "test_target[\"label\"]=[1 if i>=0 else 0 for i in test_target[\"diff\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features=train_data.drop([\"text\"],axis=1)\n",
    "test_features=test_data.drop([\"text\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features=train_features.drop([\"created_at\"],axis=1)\n",
    "test_features=test_features.drop([\"created_at\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features=test_features.drop([\"retweet_median\"],axis=1)\n",
    "test_features=test_features.drop([\"retweet_count\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features=train_features.drop([\"retweet_median\"],axis=1)\n",
    "train_features=train_features.drop([\"retweet_count\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      retweeted  screen_name           time  hashtags_num  user_mentions  \\\n",
      "0         False            0  1523297073000             0              1   \n",
      "1         False            0  1523242657000             1              0   \n",
      "10        False            0  1522729650000             1              1   \n",
      "100       False            0  1521827524000             0              1   \n",
      "1000      False            1  1517256402000             0              0   \n",
      "\n",
      "      urls  aggreg_exist  aggreg_tf  ?_exist  ?_tf     ...      people_tf  \\\n",
      "0        1             1          2        0     0     ...              0   \n",
      "1        1             0          0        0     0     ...              0   \n",
      "10       0             0          0        0     0     ...              0   \n",
      "100      1             1          1        0     0     ...              0   \n",
      "1000     1             1          1        0     0     ...              0   \n",
      "\n",
      "      retweet_exist  retweet_tf  10_exist  10_tf  you_exist  you_tf  \\\n",
      "0                 0           0         0      0          1       1   \n",
      "1                 0           0         0      0          0       0   \n",
      "10                0           0         0      0          0       0   \n",
      "100               0           0         0      0          0       0   \n",
      "1000              0           0         0      0          0       0   \n",
      "\n",
      "      top_exist  top_tf  ques_exist  \n",
      "0             0       0           0  \n",
      "1             0       0           0  \n",
      "10            0       0           0  \n",
      "100           0       0           0  \n",
      "1000          0       0           0  \n",
      "\n",
      "[5 rows x 53 columns]\n",
      "295722\n",
      "80218\n",
      "80220\n"
     ]
    }
   ],
   "source": [
    "print(train_features.head())\n",
    "print(len(train_target))\n",
    "print(len(test_features))\n",
    "print(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160314\n"
     ]
    }
   ],
   "source": [
    "print(sum([1 if i==1 else 0 for i in train_target[\"label\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5584282829290185\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "import sklearn\n",
    "clf = sklearn.ensemble.AdaBoostClassifier()  \n",
    "RandomForestModel= clf.fit(train_features,train_target[\"label\"])\n",
    "r = clf.score(test_features , test_target[\"label\"])  \n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5759738942242661\n"
     ]
    }
   ],
   "source": [
    "r2 = clf.score(train_features , train_target[\"label\"])  \n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
